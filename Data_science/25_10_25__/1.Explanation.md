# Intent Classification System Documentation

## Overview
A transformer-based intent classification system using Russian BERT (ruBERT) to categorize user queries into predefined intent categories.

## Architecture & Technical Implementation

### 1. Model Architecture

**Base Model**: `DeepPavlov/rubert-base-cased`
- **Layers**: 12 transformer layers
- **Hidden Size**: 768 dimensions
- **Attention Heads**: 12
- **Parameters**: ~178 million
- **Sequence Length**: 64 tokens (optimized for small datasets)

**Classification Head**:
- Single linear layer on top of BERT's [CLS] token
- Output dimension: `num_labels` (based on unique intents in training data)
- Activation: Softmax for probability distribution

### 2. Data Flow Pipeline

```
Raw Text → Tokenization → BERT Embeddings → Classification Layer → Softmax → Intent Prediction
```

### 3. Training Data Structure

```python
training_data = [
    ("текст запроса", "intent_label"),
    # Examples:
    ("где находится деканат", "location"),
    ("когда работает библиотека", "time"),
    ("телефон деканата", "contacts")
]
```

**Label Distribution**:
- `location`: Physical location queries
- `time`: Schedule and timing questions  
- `contacts`: Contact information requests
- `documents`: Document-related inquiries
- `payment`: Financial and payment questions
- `procedure`: Process and procedure queries
- `definition`: Definition and explanation requests
- `greeting`: Social interactions

### 4. Class Implementation Details

#### `IntentClassifier` Core Methods

**Initialization (`__init__`)**:
```python
def __init__(self, hashmap: list, epochs=5, model_name="DeepPavlov/rubert-base-cased"):
```
- `hashmap`: Training data as (text, label) pairs
- `epochs`: Training iterations (default: 5)
- `model_name`: Pre-trained transformer model

**Model Setup (`_model_setup`)**:
- Creates label mapping: `{"location": 0, "time": 1, ...}`
- Initializes BERT model with correct number of output classes
- Sets up tokenizer with Russian language support

**Tokenization (`tokenize_function`)**:
```python
def tokenize_function(self, examples):
    return self.tokenizer(
        examples['text'], 
        padding=True, 
        truncation=True, 
        max_length=64,
        return_tensors="pt"
    )
```
- **Padding**: Ensures uniform sequence length
- **Truncation**: Limits sequences to 64 tokens
- **Return Format**: PyTorch tensors for GPU compatibility

**Dataset Preparation (`set_dataset`)**:
- Converts raw data to Hugging Face Dataset format
- Applies tokenization in batches
- Maps text labels to numerical indices

### 5. Training Configuration

**Training Arguments**:
```python
training_args = TrainingArguments(
    output_dir="./intent_classifier",
    num_train_epochs=self.epochs,          # 5 epochs
    per_device_train_batch_size=8,         # Batch size
    save_steps=500,                        # Checkpoint frequency
    logging_steps=100,                     # Logging frequency
)
```

**Training Process**:
- **Optimizer**: AdamW (default in Trainer)
- **Learning Rate**: 5e-5 (default for BERT fine-tuning)
- **Loss Function**: Cross-Entropy Loss
- **Evaluation**: Training loss monitoring

### 6. Inference Pipeline

**Prediction Method**:
```python
def predict(self, text):
```
1. **Tokenization**: Same preprocessing as training
2. **Model Inference**: Forward pass through BERT + classification head
3. **Probability Calculation**: Softmax activation
4. **Result Extraction**: Argmax for predicted class

**Output Format**:
```python
return predicted_label, confidence, all_probs
```
- `predicted_label`: Most likely intent
- `confidence`: Probability score (0-1)
- `all_probs`: Complete probability distribution across all intents

### 7. Mathematical Foundation

**Softmax Function**:
```
P(y_i) = exp(z_i) / Σ(exp(z_j)) for j=1 to N
```
Where:
- `z_i` = logit for class i
- `N` = number of intent classes
- `P(y_i)` = probability of class i

**Cross-Entropy Loss**:
```
L = -Σ(y_true * log(y_pred))
```

### 8. Performance Characteristics

**Computational Requirements**:
- **Memory**: ~700MB for model + tokenizer
- **Inference Time**: ~50-100ms per query
- **Training Time**: ~2-5 minutes (5 epochs, small dataset)

**Accuracy Considerations**:
- Small dataset may lead to overfitting
- Confidence scores help assess prediction reliability
- Data augmentation recommended for production use

### 9. Model Evaluation Metrics

**Available Metrics**:
- **Accuracy**: Overall correct predictions
- **Precision**: True positives / (True positives + False positives)  
- **Recall**: True positives / (True positives + False negatives)
- **F1-Score**: Harmonic mean of precision and recall

### 10. Potential Improvements

**Data Enhancement**:
- Add more training examples per intent
- Include data augmentation (synonym replacement, back-translation)
- Balance class distribution

**Model Improvements**:
- Hyperparameter tuning (learning rate, batch size)
- Early stopping to prevent overfitting
- Class weighting for imbalanced data
- Ensemble methods

**Technical Enhancements**:
- GPU acceleration for faster training
- Model quantization for deployment
- API endpoint for real-time inference
- Continuous learning pipeline

## Usage Example

```python
# Initialize classifier
classifier = IntentClassifier(training_data, epochs=5)

# Train model
classifier.train()

# Make prediction
text = "где находится столовая?"
intent, confidence, all_probs = classifier.predict(text)

print(f"Query: {text}")
print(f"Predicted Intent: {intent}")
print(f"Confidence: {confidence:.3f}")
print(f"All Probabilities: {all_probs}")
```

This system provides a solid foundation for Russian-language intent classification and can be extended for more complex NLP tasks.