from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
from datasets import Dataset
import torch
import os

# –î–æ–±–∞–≤–ª—è–µ–º if __name__ –¥–ª—è multiprocessing
if __name__ == "__main__":
    
    training_data = [
        ("–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –¥–µ–∫–∞–Ω–∞—Ç", "location"),
        ("–∫–æ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∞", "time"),
        ("—Ç–µ–ª–µ—Ñ–æ–Ω –¥–µ–∫–∞–Ω–∞—Ç–∞", "contacts"),
        ("–Ω—É–∂–Ω–∞ —Å–ø—Ä–∞–≤–∫–∞ –æ–± –æ–±—É—á–µ–Ω–∏–∏", "documents"),
        ("—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –æ–±—É—á–µ–Ω–∏–µ", "payment"),
        ("–∫–∞–∫ –ø–æ–¥–∞—Ç—å –∑–∞—è–≤–ª–µ–Ω–∏–µ", "procedure"),
        ("—á—Ç–æ —Ç–∞–∫–æ–µ –ö–¢", "definition"),
        ("–ø—Ä–∏–≤–µ—Ç", "greeting"),
        ("–ø–æ–∫–∞", "greeting"),
        ("–∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è", "documents"),
        ("–≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã —Å—Ç–æ–ª–æ–≤–æ–π", "time"),
        ("–∞–¥—Ä–µ—Å –æ–±—â–µ–∂–∏—Ç–∏—è", "location"),
        ("–≥–¥–µ —Å—Ç–æ–ª–æ–≤–∞—è", "location"),  # –î–æ–±–∞–≤–∏–º –±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤
        ("–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å—Ç–æ–ª–æ–≤–æ–π", "time"),
        ("–∫–∞–∫ —Å–≤—è–∑–∞—Ç—å—Å—è —Å –¥–µ–∫–∞–Ω–∞—Ç–æ–º", "contacts"),
    ]

    class IntentClassifier:
        def __init__(self, hashmap: list, epochs=10, model_name="cointegrated/rubert-tiny2"):  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å
            self.model_name = model_name
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = None
            self.label_map = {}
            self.hashmap = hashmap
            self.epochs = epochs
            self.is_trained = False
            self._initialize_model()
        
        def _initialize_model(self):
            """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
            print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å...")
            texts = [itr[0] for itr in self.hashmap]
            labels = [itr[1] for itr in self.hashmap]
            
            # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
            unique_labels = sorted(list(set(labels)))
            self.label_map = {label: index for index, label in enumerate(unique_labels)}
            
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
            print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(texts)}")
            print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã: {unique_labels}")
            print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
            for label in unique_labels:
                count = labels.count(label)
                print(f"     {label}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({count/len(labels)*100:.1f}%)")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                num_labels=len(unique_labels)
            )
            print("‚úÖ –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        def tokenize_function(self, examples):
            """–§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
            return self.tokenizer(
                examples['text'], 
                padding=True, 
                truncation=True, 
                max_length=64,  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
                return_tensors="pt"
            )
        
        def prepare_data(self):
            """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
            texts = [itr[0] for itr in self.hashmap]
            labels = [itr[1] for itr in self.hashmap]
            numeric_labels = [self.label_map[label] for label in labels]
            
            dataset = Dataset.from_dict({
                'text': texts,
                'label': numeric_labels
            })
            
            tokenized_dataset = dataset.map(self.tokenize_function, batched=True)
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã: {len(tokenized_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            return tokenized_dataset
        
        def train(self):
            """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
            tokenized_dataset = self.prepare_data()
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            training_args = TrainingArguments(
                output_dir="./intent_classifier",
                num_train_epochs=self.epochs,
                per_device_train_batch_size=4,  # –£–º–µ–Ω—å—à–∞–µ–º batch size
                per_device_eval_batch_size=4,
                learning_rate=1e-4,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º learning rate
                warmup_steps=20,
                weight_decay=0.01,
                logging_dir="./logs",
                logging_steps=10,
                save_steps=100,
                evaluation_strategy="no",
                save_total_limit=1,
                report_to=None,  # –û—Ç–∫–ª—é—á–∞–µ–º –æ—Ç—á–µ—Ç—ã
                dataloader_pin_memory=False,  # –î–ª—è Windows
            )
            
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=tokenized_dataset,
            )
            
            print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
            train_result = trainer.train()
            trainer.save_model()
            self.is_trained = True
            
            print("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è:")
            print(f"   Final loss: {train_result.metrics['train_loss']:.4f}")
            print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            return trainer
        
        def predict(self, text):
            """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
            if not self.is_trained:
                raise ValueError("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å!")
            
            inputs = self.tokenizer(
                text,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=64
            )
            
            self.model.eval()
            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.softmax(outputs.logits, dim=1)
                predicted_idx = torch.argmax(probs, dim=1).item()
                confidence = probs[0][predicted_idx].item()
            
            reverse_map = {v: k for k, v in self.label_map.items()}
            predicted_label = reverse_map[predicted_idx]
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏
            all_probs = {reverse_map[i]: float(probs[0][i]) for i in range(len(reverse_map))}
            
            return predicted_label, confidence, all_probs

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
    print("üéØ –ó–ê–ü–£–°–ö –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê –ù–ê–ú–ï–†–ï–ù–ò–ô")
    print("=" * 50)
    
    classifier = IntentClassifier(training_data, epochs=15)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —ç–ø–æ—Ö–∏
    classifier.train()
    
    print("\nüß™ –¢–ï–°–¢–ò–†–£–ï–ú –ú–û–î–ï–õ–¨:")
    print("=" * 50)
    
    test_questions = [
        "–≥–¥–µ —Å—Ç–æ–ª–æ–≤–∞—è?", 
        "–∫–∞–∫–æ–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç—ã?", 
        "–ø—Ä–∏–≤–µ—Ç!",
        "–º–Ω–µ –Ω—É–∂–Ω–∞ —Å–ø—Ä–∞–≤–∫–∞",
        "–∞–¥—Ä–µ—Å –¥–µ–∫–∞–Ω–∞—Ç–∞",
        "—Ç–µ–ª–µ—Ñ–æ–Ω –±–∏–±–ª–∏–æ—Ç–µ–∫–∏"
    ]
    
    for question in test_questions:
        try:
            intent, confidence, all_probs = classifier.predict(question)
            print(f"\nüìù –í–æ–ø—Ä–æ—Å: '{question}'")
            print(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {intent} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f})")
            print(f"üìä –í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:")
            for label, prob in sorted(all_probs.items(), key=lambda x: x[1], reverse=True):
                print(f"   {label}: {prob:.3f}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è '{question}': {e}")