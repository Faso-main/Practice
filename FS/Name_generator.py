import fasttext
import numpy as np
from sklearn.cluster import DBSCAN, KMeans
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import re, os

class SmartFastTextGrouper:
    def __init__(self, model_path=os.path.join('FS','cc.ru.300.bin')):
        self.model = fasttext.load_model(model_path)
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –≥—Ä—É–ø–ø
        self.category_map = {
            '–∫–æ–º–ø—å—é—Ç–µ—Ä': '–ö–æ–º–ø—å—é—Ç–µ—Ä—ã',
            '–Ω–æ—É—Ç–±—É–∫': '–ù–æ—É—Ç–±—É–∫–∏', 
            '–º–æ–Ω–∏—Ç–æ—Ä': '–ú–æ–Ω–∏—Ç–æ—Ä—ã',
            '–ø—Ä–∏–Ω—Ç–µ—Ä': '–ü—Ä–∏–Ω—Ç–µ—Ä—ã',
            '–∫–∞—Ä—Ç—Ä–∏–¥–∂': '–ö–∞—Ä—Ç—Ä–∏–¥–∂–∏',
            '–º—ã—à—å': '–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –º—ã—à–∏',
            '–∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞': '–ö–ª–∞–≤–∏–∞—Ç—É—Ä—ã',
            '–≤–µ–±–∫–∞–º–µ—Ä–∞': '–í–µ–±-–∫–∞–º–µ—Ä—ã',
            '—Å—Ç—É–ª': '–û—Ñ–∏—Å–Ω—ã–µ —Å—Ç—É–ª—å—è',
            '—Å—Ç–æ–ª': '–°—Ç–æ–ª—ã',
            '–º–µ–±–µ–ª—å': '–û—Ñ–∏—Å–Ω–∞—è –º–µ–±–µ–ª—å',
            '–∫–∞–Ω—Ü–µ–ª—è—Ä—Å–∫–∏–π': '–ö–∞–Ω—Ü–µ–ª—è—Ä–∏—è',
            '–±—É–º–∞–≥–∞': '–ë—É–º–∞–≥–∞',
            '–ø—Ä–æ–µ–∫—Ç–æ—Ä': '–ü—Ä–æ–µ–∫—Ç–æ—Ä—ã',
            '—Ä–æ—É—Ç–µ—Ä': '–°–µ—Ç–µ–≤–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ'
        }
    
    def preprocess_text(self, text):
        text = text.lower()
        text = re.sub(r'[^–∞-—è—ë\s]', ' ', text)
        return ' '.join(text.split())
    
    def get_text_vector(self, text):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ —Å —É—á–µ—Ç–æ–º –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤"""
        clean_text = self.preprocess_text(text)
        words = clean_text.split()
        
        # –í–∑–≤–µ—à–∏–≤–∞–µ–º —Å–ª–æ–≤–∞ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        word_vectors = []
        for word in words:
            if len(word) > 2 and word in self.model.words:
                vector = self.model.get_word_vector(word)
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–≤
                if word in self.category_map:
                    word_vectors.extend([vector] * 3)  # –¢—Ä–æ–π–Ω–æ–π –≤–µ—Å –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                else:
                    word_vectors.append(vector)
        
        if word_vectors:
            return np.mean(word_vectors, axis=0)
        return np.zeros(300)
    
    def detect_main_category(self, text):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–µ–∫—Å—Ç–∞"""
        clean_text = self.preprocess_text(text)
        words = clean_text.split()
        
        for word in words:
            if word in self.category_map:
                return self.category_map[word]
        
        return None
    
    def group_texts_improved(self, texts):
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–µ–π
        """
        print(f"üéØ –ì—Ä—É–ø–ø–∏—Ä—É–µ–º {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
        
        # –®–∞–≥ 1: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categorized = {}
        uncategorized = []
        
        for text in texts:
            category = self.detect_main_category(text)
            if category:
                categorized.setdefault(category, []).append(text)
            else:
                uncategorized.append(text)
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(categorized)}")
        print(f"   –ù–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(uncategorized)}")
        
        # –®–∞–≥ 2: –î–µ—Ç–∞–ª—å–Ω–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        final_groups = {}
        
        for category, items in categorized.items():
            if len(items) <= 3:
                # –ú–∞–ª–µ–Ω—å–∫–∏–µ –≥—Ä—É–ø–ø—ã –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                final_groups[category] = items
            else:
                # –ë–æ–ª—å—à–∏–µ –≥—Ä—É–ø–ø—ã —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–æ–¥–≥—Ä—É–ø–ø—ã
                subgroups = self._cluster_within_category(items)
                final_groups.update(subgroups)
        
        # –®–∞–≥ 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        if uncategorized:
            uncat_groups = self._cluster_uncategorized(uncategorized)
            final_groups.update(uncat_groups)
        
        return dict(sorted(final_groups.items(), key=lambda x: len(x[1]), reverse=True))
    
    def _cluster_within_category(self, items):
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if len(items) <= 2:
            return {self._generate_detailed_name(items): items}
        
        vectors = np.array([self.get_text_vector(text) for text in items])
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º DBSCAN –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        clustering = DBSCAN(eps=0.3, min_samples=2, metric='cosine').fit(vectors)
        labels = clustering.labels_
        
        groups = {}
        for label, text in zip(labels, items):
            if label == -1:  # –í—ã–±—Ä–æ—Å—ã
                groups.setdefault('–†–∞–∑–Ω–æ–µ', []).append(text)
            else:
                groups.setdefault(label, []).append(text)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–¥–≥—Ä—É–ø–ø
        result = {}
        for group_items in groups.values():
            if len(group_items) >= 2:
                name = self._generate_detailed_name(group_items)
                result[name] = group_items
            else:
                result.setdefault('–†–∞–∑–Ω–æ–µ', []).extend(group_items)
        
        return result
    
    def _cluster_uncategorized(self, items):
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        if len(items) <= 3:
            return {'–†–∞–∑–Ω–æ–µ': items}
        
        vectors = np.array([self.get_text_vector(text) for text in items])
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        n_clusters = min(4, len(items) // 2)
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(vectors)
        
        groups = {}
        for label, text in zip(labels, items):
            groups.setdefault(label, []).append(text)
        
        result = {}
        for group_items in groups.values():
            name = self._generate_detailed_name(group_items)
            result[name] = group_items
        
        return result
    
    def _generate_detailed_name(self, items):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –≥—Ä—É–ø–ø—ã"""
        if len(items) == 1:
            words = self.preprocess_text(items[0]).split()[:3]
            return ' '.join(words).title()
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞
        all_words = []
        for text in items:
            words = self.preprocess_text(text).split()
            all_words.extend([w for w in words if len(w) > 2])
        
        word_counts = Counter(all_words)
        
        # –ò—â–µ–º —Å–∞–º—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ (–Ω–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞)
        stop_words = {'–¥–ª—è', '–∏', '–≤', '–Ω–∞', '—Å', '–∏–∑', '–æ—Ç', '–¥–æ', '–ø–æ', '–∑–∞'}
        keywords = []
        
        for word, count in word_counts.most_common(10):
            if (word not in stop_words and 
                word not in keywords and
                count >= max(2, len(items) // 3)):  # –°–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–µ–∫—Å—Ç–æ–≤
                keywords.append(word)
            
            if len(keywords) >= 2:
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ö–æ—Ä–æ—à–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞
        if not keywords:
            first_text_words = self.preprocess_text(items[0]).split()
            keywords = [w for w in first_text_words if len(w) > 2][:2]
        
        name = ' '.join(keywords).title() if keywords else '–†–∞–∑–Ω–æ–µ'
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ—Å–ª–∏ –≥—Ä—É–ø–ø–∞ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è
        if len(items) > 2:
            name += f' ({len(items)})'
        
        return name

# –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ì–û –ê–õ–ì–û–†–ò–¢–ú–ê
if __name__ == '__main__':
    grouper = SmartFastTextGrouper()
    
    test_data = [
        "–ó–∞–∫—É–ø–∫–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤ –∏ –æ—Ä–≥—Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è –æ—Ñ–∏—Å–∞",
        "–ù–æ—É—Ç–±—É–∫–∏ Dell Latitude –¥–ª—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤", 
        "–ú–æ–Ω–∏—Ç–æ—Ä—ã Samsung 24 –¥—é–π–º–∞",
        "–û—Ñ–∏—Å–Ω—ã–µ —Å—Ç—É–ª—å—è —ç—Ä–≥–æ–Ω–æ–º–∏—á–Ω—ã–µ",
        "–°—Ç–æ–ª—ã –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ —Ä–µ–≥—É–ª–∏—Ä—É–µ–º—ã–µ",
        "–ö–∞–Ω—Ü–µ–ª—è—Ä—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã –¥–ª—è –æ—Ç–¥–µ–ª–∞",
        "–ë—É–º–∞–≥–∞ –¥–ª—è –ø—Ä–∏–Ω—Ç–µ—Ä–∞ –ê4",
        "–ü—Ä–∏–Ω—Ç–µ—Ä—ã –ª–∞–∑–µ—Ä–Ω—ã–µ HP",
        "–ú—ã—à–∏ –∏ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã –±–µ—Å–ø—Ä–æ–≤–æ–¥–Ω—ã–µ",
        "–í–µ–±-–∫–∞–º–µ—Ä—ã –¥–ª—è –≤–∏–¥–µ–æ–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–π",
        "–°–∏—Å—Ç–µ–º–Ω—ã–µ –±–ª–æ–∫–∏ –¥–ª—è —Ä–∞–±–æ—á–µ–≥–æ –º–µ—Å—Ç–∞",
        "–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–µ –º—ã—à–∏ Logitech",
        "–ö–∞—Ä—Ç—Ä–∏–¥–∂–∏ –¥–ª—è –ø—Ä–∏–Ω—Ç–µ—Ä–∞",
        "–û—Ñ–∏—Å–Ω–∞—è –º–µ–±–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–Ω–æ–π",
        "–ü—Ä–æ–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü-–∑–∞–ª–∞",
        "–†–æ—É—Ç–µ—Ä—ã –∏ —Å–µ—Ç–µ–≤–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ"
    ]
    
    print("=" * 70)
    print("–£–õ–£–ß–®–ï–ù–ù–ê–Ø –ì–†–£–ü–ü–ò–†–û–í–ö–ê –° –ö–ê–¢–ï–ì–û–†–ò–ó–ê–¶–ò–ï–ô")
    print("=" * 70)
    
    groups = grouper.group_texts_improved(test_data)
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"–í—Å–µ–≥–æ –≥—Ä—É–ø–ø: {len(groups)}")
    
    for name, items in groups.items():
        print(f"\nüè∑Ô∏è  {name}:")
        for item in items:
            print(f"   ‚Ä¢ {item}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    group_sizes = [len(items) for items in groups.values()]
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –†–∞–∑–º–µ—Ä—ã –≥—Ä—É–ø–ø: {group_sizes}")
    print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {np.mean(group_sizes):.1f}")
    print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min(group_sizes)}")
    print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max(group_sizes)}")